{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#acknowledgement\n",
    "#https://github.com/nsanghi/HSE-NLP-Coursera/blob/master/week1/week1-MultilabelClassification-Solution.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_corpus(file_path):\n",
    "    \"\"\" \n",
    "        file_path: the text file path of the corpus\n",
    "        return: list of lines.\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    with open(file_path,'r') as f:\n",
    "        for line in f:\n",
    "            result.append(line)\n",
    "            \n",
    "    return result\n",
    "            \n",
    "def construct_vocab_counts(list_of_lines):\n",
    "    \"\"\"\n",
    "        list_of_lines: list of tokenized lines\n",
    "        \n",
    "        return: Counter of tokens\n",
    "    \"\"\"\n",
    "    from collections import Counter\n",
    "    words_counts = Counter([word for line in list_of_lines for word in line.split(' ')])\n",
    "    return words_counts\n",
    "\n",
    "def construct_vocab_list(words_counts, min_thres=0):\n",
    "    \"\"\"\n",
    "        words_counts: counter of words\n",
    "        min_thres: word occuring less than min_thres times will not be in the vocab\n",
    "    \"\"\"\n",
    "    vocab_list = [x for x in words_counts.keys() if words_counts[x] >= min_thres]\n",
    "    if 'UNK' not in vocab_list:\n",
    "        vocab_list.append('<UNK>')\n",
    "    return vocab_list\n",
    "\n",
    "def construct_vocab_dict(vocab_list):\n",
    "    \"\"\"\n",
    "        vocab_list: list of vocabularies\n",
    "        return: two dictionaries of mappings between words and indices\n",
    "    \"\"\"\n",
    "    word_to_idx_dict = {item:ii for ii, item in enumerate(vocab_list)}\n",
    "    idx_to_word_dict = {ii:word for word, ii in word_to_idx_dict.items()}\n",
    "    \n",
    "    return word_to_idx_dict, idx_to_word_dict\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_to_word_indices(list_of_lines, word_to_idx_dict, train=True):\n",
    "    if train:\n",
    "        result = [word_to_idx_dict[word] for line in list_of_lines for word in line.split(' ')]\n",
    "    else:\n",
    "        result = [word_to_idx_dict[word] if word in word_to_idx_dict.keys() else word_to_idx_dict['<UNK>']\n",
    "                for line in list_of_lines for word in line.split(' ')]\n",
    "        \n",
    "    return result\n",
    "\n",
    "def write_to_file(result, file_name):\n",
    "    pickle.dump(result, open(file_name, 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading file...\n",
      "counting words...\n",
      "producing vocabulary list...\n",
      "vocan list contains 17863 words\n",
      "showing the first ten indices:\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "reconstruct token from the above indices:\n",
      "<s> Kick @-@ Ass is a 2010 superhero black comedy\n",
      "writing corpus word indices to file...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "infile = '../group4.test.txt'\n",
    "outfile = '../testing_step2.p'\n",
    "train='test'\n",
    "print(\"loading file...\")\n",
    "lines = load_corpus(infile)\n",
    "print(\"counting words...\")\n",
    "words_counts = construct_vocab_counts(lines)\n",
    "print(\"producing vocabulary list...\")\n",
    "vocab_list = construct_vocab_list(words_counts, min_thres=0)\n",
    "print(\"vocan list contains {} words\".format(len(vocab_list)))\n",
    "wi_dict, iw_dict = construct_vocab_dict(vocab_list)\n",
    "corpus_word_indices = corpus_to_word_indices(lines,wi_dict, train=='train')\n",
    "print(\"showing the first ten indices:\")\n",
    "print(corpus_word_indices[:10])\n",
    "print(\"reconstruct token from the above indices:\")\n",
    "print(' '.join([iw_dict[i] for i in corpus_word_indices[:10]]))\n",
    "print(\"writing corpus word indices to file...\")\n",
    "write_to_file(corpus_word_indices, outfile)\n",
    "print(\"done\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 13, 17, 18]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_word_indices[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> Kick @-@ Ass is a 2010 superhero black comedy film based on the comic book of the same name by Mark Millar and John Romita , Jr . </s> <s> which was published by Marvel Comics . </s> <s>'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([iw_dict[i] for i in corpus_word_indices[:40]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
