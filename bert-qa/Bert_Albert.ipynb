{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bert_Albert.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "xIO_ieDmyJ8j",
        "cPeTWwSursk2"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7bacf37ec1754169ba817424bcbe8afd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_57507aaa576748ada0df8483165970ba",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d9b2eb920b7d4b3b8ddd266d8afcb638",
              "IPY_MODEL_f3aa5d79d2454952966614ac5f18bcea"
            ]
          }
        },
        "57507aaa576748ada0df8483165970ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d9b2eb920b7d4b3b8ddd266d8afcb638": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e60c40b9f8ba4c1db0c4408dec8ee49a",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 463,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 463,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5ebffae612cf4c35ac5a23d82dbf5498"
          }
        },
        "f3aa5d79d2454952966614ac5f18bcea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8805a41eea674c55994978961e40f3f0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 463/463 [00:01&lt;00:00, 270B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_61b8e7ab25bc4ff1b05735dfa6f62b64"
          }
        },
        "e60c40b9f8ba4c1db0c4408dec8ee49a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5ebffae612cf4c35ac5a23d82dbf5498": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8805a41eea674c55994978961e40f3f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "61b8e7ab25bc4ff1b05735dfa6f62b64": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5ef13396669a4540a044a3deef4be3bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_9d72066db34841b089512a66b14ef620",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_742b1cd42808450f90b8494c6e6a5fcd",
              "IPY_MODEL_14deb3f99ce84a218ab77902d25cbb7e"
            ]
          }
        },
        "9d72066db34841b089512a66b14ef620": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "742b1cd42808450f90b8494c6e6a5fcd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_7f0f429ff24745f2ba57b7c10a5876ea",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 44701420,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 44701420,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_eac68c81bd8c451ca2f60b2e199238c9"
          }
        },
        "14deb3f99ce84a218ab77902d25cbb7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_364600b187bf4404a1c1707aa1471263",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 44.7M/44.7M [00:02&lt;00:00, 15.3MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7214e5f975864cf6805bbb749f4b8dea"
          }
        },
        "7f0f429ff24745f2ba57b7c10a5876ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "eac68c81bd8c451ca2f60b2e199238c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "364600b187bf4404a1c1707aa1471263": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7214e5f975864cf6805bbb749f4b8dea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "896bf9c054c14fd5bb4417fa10d07b7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_15762e79cd2f40f1a56612e4a4b9d64c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_696e6945f71948b2bcad15440a43fb26",
              "IPY_MODEL_0d6293fe3743410ca5d20600265526a2"
            ]
          }
        },
        "15762e79cd2f40f1a56612e4a4b9d64c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "696e6945f71948b2bcad15440a43fb26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e2925c97e16143e5991f3d570cae8306",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_143d521a9337483e9399e45e9e582981"
          }
        },
        "0d6293fe3743410ca5d20600265526a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_db16a09c645e4d8cbaf8e4b4fb1ef533",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:05&lt;00:00, 41.4kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1e68ba5d26f9498e9f2383783479ec32"
          }
        },
        "e2925c97e16143e5991f3d570cae8306": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "143d521a9337483e9399e45e9e582981": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "db16a09c645e4d8cbaf8e4b4fb1ef533": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1e68ba5d26f9498e9f2383783479ec32": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f47b234db09e4d24a215b1d724b8a33d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_8456925f82d544ceab63087e3bea237c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_9fba6f0fddd245979c0d2944ae562b2c",
              "IPY_MODEL_bf95401a56034146a5df9c33a099ba03"
            ]
          }
        },
        "8456925f82d544ceab63087e3bea237c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9fba6f0fddd245979c0d2944ae562b2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9165c560f9e84030b9e7815e702f0591",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 112,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 112,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_493d2fb59a52406a9b2c1b5c070e5d4e"
          }
        },
        "bf95401a56034146a5df9c33a099ba03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3b07b8e1c87a487c9e3e6ba6bd098124",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 112/112 [00:01&lt;00:00, 60.0B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_deb5712b4ede49c499ee0df45d2b0105"
          }
        },
        "9165c560f9e84030b9e7815e702f0591": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "493d2fb59a52406a9b2c1b5c070e5d4e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3b07b8e1c87a487c9e3e6ba6bd098124": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "deb5712b4ede49c499ee0df45d2b0105": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2746947dda1c4e36a1430b0088cd56ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_87956815b2884d8297fff8382d2d016a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_5924a76b44f2465086c7017de92ae79c",
              "IPY_MODEL_454c7066d854477e9ea2eef6b078a356"
            ]
          }
        },
        "87956815b2884d8297fff8382d2d016a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5924a76b44f2465086c7017de92ae79c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_7781c3d935e24d5fb950cfd52ac398ef",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 23,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 23,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_df59e5787376408285c493e12cdee002"
          }
        },
        "454c7066d854477e9ea2eef6b078a356": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8923c18d1fb44da7a4bd776e1f01d997",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 23.0/23.0 [00:00&lt;00:00, 338B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d9e1f61447204ad9ba5ce4ac4f8f4c59"
          }
        },
        "7781c3d935e24d5fb950cfd52ac398ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "df59e5787376408285c493e12cdee002": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8923c18d1fb44da7a4bd776e1f01d997": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d9e1f61447204ad9ba5ce4ac4f8f4c59": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxyizlCYmgJs",
        "colab_type": "text"
      },
      "source": [
        "### Loading models and Setting up ensembling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZOh_nPSIFRj",
        "colab_type": "text"
      },
      "source": [
        "#### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2odGUNqioXW",
        "colab_type": "code",
        "outputId": "73277233-e81c-41db-ce9e-ac3178688df4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/35/ad2c5b1b8f99feaaf9d7cdadaeef261f098c6e1a6a2935d4d07662a6b780/transformers-2.11.0-py3-none-any.whl (674kB)\n",
            "\r\u001b[K     |▌                               | 10kB 23.8MB/s eta 0:00:01\r\u001b[K     |█                               | 20kB 29.2MB/s eta 0:00:01\r\u001b[K     |█▌                              | 30kB 22.0MB/s eta 0:00:01\r\u001b[K     |██                              | 40kB 15.1MB/s eta 0:00:01\r\u001b[K     |██▍                             | 51kB 12.9MB/s eta 0:00:01\r\u001b[K     |███                             | 61kB 12.7MB/s eta 0:00:01\r\u001b[K     |███▍                            | 71kB 12.7MB/s eta 0:00:01\r\u001b[K     |███▉                            | 81kB 11.2MB/s eta 0:00:01\r\u001b[K     |████▍                           | 92kB 11.1MB/s eta 0:00:01\r\u001b[K     |████▉                           | 102kB 11.0MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 112kB 11.0MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 122kB 11.0MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 133kB 11.0MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 143kB 11.0MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 153kB 11.0MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 163kB 11.0MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 174kB 11.0MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 184kB 11.0MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 194kB 11.0MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 204kB 11.0MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 215kB 11.0MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 225kB 11.0MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 235kB 11.0MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 245kB 11.0MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 256kB 11.0MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 266kB 11.0MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 276kB 11.0MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 286kB 11.0MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 296kB 11.0MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 307kB 11.0MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 317kB 11.0MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 327kB 11.0MB/s eta 0:00:01\r\u001b[K     |████████████████                | 337kB 11.0MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 348kB 11.0MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 358kB 11.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 368kB 11.0MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 378kB 11.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 389kB 11.0MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 399kB 11.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 409kB 11.0MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 419kB 11.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 430kB 11.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 440kB 11.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 450kB 11.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 460kB 11.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 471kB 11.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 481kB 11.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 491kB 11.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 501kB 11.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 512kB 11.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 522kB 11.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 532kB 11.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 542kB 11.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 552kB 11.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 563kB 11.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 573kB 11.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 583kB 11.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 593kB 11.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 604kB 11.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 614kB 11.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 624kB 11.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 634kB 11.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 645kB 11.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 655kB 11.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 665kB 11.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 675kB 11.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 48.7MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 52.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Collecting tokenizers==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 52.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=c2c861539827e9d8aa552f31693cedbffe07be230e8929b56dd7e383d57190e2\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.7.0 transformers-2.11.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ms3JkZd6y8oe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import json\n",
        "from google.colab import output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXwA3brpJol6",
        "colab_type": "text"
      },
      "source": [
        "#### Prediction functions\n",
        "\n",
        "That's where the imported models go.\n",
        "Calls to ensembling methods\n",
        "\n",
        "The encode_plus() method is a shortcut and you can expect other models not to have such help. So ensembling functions may need to be edited a little."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfhppSkvK1AO",
        "colab_type": "text"
      },
      "source": [
        "##### Triple"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcG1QItEkCjt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ensemble_triple(model_x, model_y, model_z, tokenizer_x, tokenizer_y, tokenizer_z, question, answer_text, alpha=.5):\n",
        "    \"\"\"\n",
        "    Combines outputs from three transformers.\n",
        "    NOTE: if tokenizer doesn't encode into a tuple / dict then make sure\n",
        "    to unpack dictionary, input id's and input mask.\n",
        "    Alpha must be smaller than .5.\n",
        "    \"\"\"\n",
        "\n",
        "    # ======== Automated solution ========\n",
        "    # Get token segment id's with encode plus\n",
        "    max_len = 300\n",
        "\n",
        "    input_dict_x = tokenizer_x.encode_plus(question, answer_text, max_length=max_len, pad_to_max_length=True, return_tensors='pt')\n",
        "    input_dict_y = tokenizer_y.encode_plus(question, answer_text, max_length=max_len, pad_to_max_length=True, return_tensors='pt')\n",
        "    input_dict_z = tokenizer_z.encode_plus(question, answer_text, max_length=max_len, pad_to_max_length=True, return_tensors='pt')\n",
        "\n",
        "    # ======== Evaluate ========\n",
        "    # Run our example question through the model.\n",
        "    start_scores_x, end_scores_x = model_x(**input_dict_x)\n",
        "    start_scores_y, end_scores_y = model_y(**input_dict_y)\n",
        "    start_scores_z, end_scores_z = model_z(**input_dict_z)\n",
        "\n",
        "    start = []\n",
        "    start.append(torch.nn.functional.softmax(start_scores_x, dim=1).detach().numpy()[0])\n",
        "    start.append(torch.nn.functional.softmax(start_scores_y, dim=1).detach().numpy()[0])\n",
        "    start.append(torch.nn.functional.softmax(start_scores_z, dim=1).detach().numpy()[0])\n",
        "\n",
        "    end = []\n",
        "    end.append(torch.nn.functional.softmax(end_scores_x, dim=1).detach().numpy()[0])\n",
        "    end.append(torch.nn.functional.softmax(end_scores_y, dim=1).detach().numpy()[0])\n",
        "    end.append(torch.nn.functional.softmax(end_scores_z, dim=1).detach().numpy()[0])\n",
        "\n",
        "    # model, start_ind, end_ind = ensemble_confidence_prob(start, end)\n",
        "    # model, start_ind, end_ind = ensemble_confidence_score(start, end)\n",
        "\n",
        "\n",
        "    # print(\"ANSWER: %d %d %d\" %(model, start_ind, end_ind))\n",
        "\n",
        "    # ======== Reconstruct Answer ========\n",
        "    # Find the tokens with the highest `start` and `end` scores.\n",
        "    # RANKS\n",
        "    # mode = {'equal', 'ranks', 'f1'}\n",
        "    model, answer_start, answer_end = combine_scores2(start, end, mode=\"equal\")\n",
        "    \n",
        "    # There's a chance this might hit index out of bounds\n",
        "    if model == 0:\n",
        "      answer = tokenizer_x.decode(input_dict_x['input_ids'][0, answer_start:answer_end+1], skip_special_tokens=True)\n",
        "    elif model == 1:\n",
        "      answer = tokenizer_y.decode(input_dict_y['input_ids'][0, answer_start:answer_end+1], skip_special_tokens=True)\n",
        "    else:\n",
        "      answer = tokenizer_z.decode(input_dict_z['input_ids'][0, answer_start:answer_end+1], skip_special_tokens=True)\n",
        "    # print('Answer: \"' + answer + '\"')\n",
        "    return answer\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGFwihmkLcCH",
        "colab_type": "text"
      },
      "source": [
        "##### Double"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqmL6C7AKzrk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ensemble_double(model_x, model_y, tokenizer_x, tokenizer_y, question, answer_text, alpha=.5):\n",
        "    \"\"\"\n",
        "    Combine two models\n",
        "    \"\"\"\n",
        "\n",
        "    # ======== Automated solution ========\n",
        "    # Get token segment id's with encode plus \n",
        "    ## removed arg: pad_to_max_length=True\n",
        "    input_dict_x = tokenizer_x.encode_plus(question, answer_text, return_tensors='pt')\n",
        "    input_dict_y = tokenizer_y.encode_plus(question, answer_text, return_tensors='pt')\n",
        "\n",
        "    # ======== Evaluate ========\n",
        "    # Run our example question through the model.\n",
        "    start_scores_x, end_scores_x = model_x(**input_dict_x)\n",
        "    start_scores_y, end_scores_y = model_y(**input_dict_y)\n",
        "\n",
        "    start = []\n",
        "    start.append(torch.nn.functional.softmax(start_scores_x, dim=1).detach().numpy()[0])\n",
        "    start.append(torch.nn.functional.softmax(start_scores_y, dim=1).detach().numpy()[0])\n",
        "\n",
        "    end = []\n",
        "    end.append(torch.nn.functional.softmax(end_scores_x, dim=1).detach().numpy()[0])\n",
        "    end.append(torch.nn.functional.softmax(end_scores_y, dim=1).detach().numpy()[0])\n",
        "\n",
        "    # model, start_ind, end_ind = ensemble_confidence_prob(start, end)\n",
        "    # model, start_ind, end_ind = ensemble_confidence_score(start, end)\n",
        "\n",
        "\n",
        "    # print(\"ANSWER: %d %d %d\" %(model, start_ind, end_ind))\n",
        "\n",
        "    # ======== Reconstruct Answer ========\n",
        "    # Find the tokens with the highest `start` and `end` scores.\n",
        "    # mode = {'equal', 'ranks', 'f1'}\n",
        "    model, answer_start, answer_end = combine_scores2(start, end, mode=\"equal\")\n",
        "    \n",
        "    # There's a chance this might hit index out of bounds\n",
        "    if model == 0:\n",
        "      answer = tokenizer_x.decode(input_dict_x['input_ids'][0, answer_start:answer_end+1], skip_special_tokens=True)\n",
        "    else:\n",
        "      answer = tokenizer_y.decode(input_dict_y['input_ids'][0, answer_start:answer_end+1], skip_special_tokens=True)\n",
        "    # print('Answer: \"' + answer + '\"')\n",
        "    return answer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWX-3gB7MCdB",
        "colab_type": "text"
      },
      "source": [
        "#### Ensembling methods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfFco4cJWbPT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ensemble_confidence_score(start_scores, end_scores):\n",
        "  prob, answer_start, answer_end, model = 0, 0, 0, 0\n",
        "  for i in range(len(start_scores)):\n",
        "    start = np.argmax(start_scores[i])\n",
        "    end = np.argmax(end_scores[i])\n",
        "    if (start_scores[i][start]*end_scores[i][end]) > prob:\n",
        "      prob = start_scores[i][start]*end_scores[i][end]\n",
        "      model = i \n",
        "      answer_start = start \n",
        "      answer_end = end \n",
        "\n",
        "  return model, answer_start, answer_end\n",
        "\n",
        "def ensemble_confidence_prob(start_prob, end_prob):\n",
        "  prob, answer_start, answer_end, model = 0.0, 0, 0, 0\n",
        "  for i in range(len(start_prob)):\n",
        "    # print(\"%f %f \" %(max(start_prob[i]), max(end_prob[i])))\n",
        "    for j in range(len(start_prob[i])):\n",
        "      temp = start_prob[i][j] * end_prob[i]\n",
        "      if max(temp) > prob:\n",
        "        prob = max(temp)\n",
        "        model = i\n",
        "        answer_start = j\n",
        "        answer_end = torch.argmax(torch.Tensor(temp))\n",
        "    # print(\"%d %d %d %f\" %(model, answer_start, answer_end, prob))\n",
        "\n",
        "  return model, answer_start, answer_end\n",
        "      \n",
        "\n",
        "  # ans = torch.mm(torch.transpose(torch.Tensor(start_prob), 0, 1), torch.Tensor(end_prob))\n",
        "  # print(ans)\n",
        "  # return model, answer_start, answer_end\n",
        "\n",
        "def ensemble_weighted_average_score(ranks, start_scores, end_scores):\n",
        "  # use F1 ranks\n",
        "  prob, answer_start, answer_end, model = 0.0, 0, 0, 0\n",
        "  for i in range(len(start_scores)):\n",
        "    start = np.argmax(start_scores[i])\n",
        "    end = np.argmax(end_scores[i])\n",
        "    if (ranks[i] * start_scores[i][start]*end_scores[i][end]) > prob:\n",
        "      prob = ranks[i] * start_scores[i][start]*end_scores[i][end]\n",
        "      model = i \n",
        "      answer_start = start \n",
        "      answer_end = end \n",
        "    # print(\"%d %d %d %f\" %(model, answer_start, answer_end, prob))\n",
        "\n",
        "  return model, answer_start, answer_end\n",
        "\n",
        "def ensemble_weighted_average_prob(ranks, start_prob, end_prob):\n",
        "  # use F1 ranks\n",
        "  prob, answer_start, answer_end, model = 0.0, 0, 0, 0\n",
        "  for i in range(len(start_prob)):\n",
        "    for j in range(len(start_prob[i])):\n",
        "      temp = ranks[i] * start_prob[i][j] * end_prob[i]\n",
        "      if max(temp) > prob:\n",
        "        prob = max(temp)\n",
        "        model = i\n",
        "        answer_start = j\n",
        "        answer_end = torch.argmax(torch.Tensor(temp))\n",
        "    # print(\"%d %d %d %f\" %(model, answer_start, answer_end, prob))\n",
        "\n",
        "  return model, answer_start, answer_end"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JBD8i_yUDFx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def add_lists(tup):\n",
        "  ans = tup[0]\n",
        "\n",
        "  for i in range(1, len(tup)):\n",
        "    np.add(ans, tup[i])\n",
        "\n",
        "  return ans\n",
        "\n",
        "def ensemble_weighted_average_new(ranks, start_prob, end_prob):\n",
        "  prob, answer_start, answer_end, max_model, ans_val, max_start, max_end = [], 0, 0, 0, 0.0, 0, 0\n",
        "  # number of models\n",
        "  for i in range(len(start_prob)):\n",
        "    # number of spans\n",
        "    temp = []\n",
        "    for j in range(len(start_prob[i])):\n",
        "      t = ranks[i] * start_prob[i][j] * np.asarray(end_prob[i])\n",
        "      temp.append(t)\n",
        "      temp_val = max(t)\n",
        "      if temp_val > ans_val:\n",
        "        ans_val = temp_val \n",
        "        max_model = i\n",
        "        max_start = j\n",
        "        max_end = np.where(t == temp_val)\n",
        "    \n",
        "    prob.append(temp)\n",
        "\n",
        "  # print(prob )\n",
        "  answer_mat = np.asarray(prob[0])\n",
        "  for i in range(1, len(prob)):\n",
        "    answer_mat = np.add(answer_mat, np.asarray(prob[i]))\n",
        "\n",
        "  # print(answer_mat)\n",
        "  # print(answer_mat) \n",
        "  a = np.amax(answer_mat)\n",
        "  answer_start = np.where(answer_mat == a)\n",
        "  answer_end = int(answer_start[1][0])\n",
        "  answer_start = int(answer_start[0][0])\n",
        "    # print(\"%d %d %d %f\" %(model, answer_start, answer_end, prob))\n",
        "\n",
        "  return max_model, max_start, max_end, answer_start, answer_end"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpufO7gwaPJ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def combine_scores2(start_scores, end_scores, mode=\"equal\"):\n",
        "  if mode == \"equal\":\n",
        "    l = len(start_scores)\n",
        "    ranks = [1.0/l] * l\n",
        "    model, max_start, max_end, start, end = ensemble_weighted_average_new(ranks, start_scores, end_scores)\n",
        "    return model, start, end\n",
        "\n",
        "  elif mode == \"ranks\":\n",
        "    l = len(start_scores)\n",
        "    ranks = list(range(1, l+1))\n",
        "    l = l*(l+1)//2\n",
        "    ranks = [x/float(l) for x in ranks]\n",
        "    model, max_start, max_end, start, end = ensemble_weighted_average_new(ranks, start_scores, end_scores)\n",
        "    return model, start, end\n",
        "\n",
        "  elif mode == \"f1\":\n",
        "    ranks = []\n",
        "    model, max_start, max_end, start, end = ensemble_weighted_average_new(ranks, start_scores, end_scores)\n",
        "    return model, start, end"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qK483mGEJFO2",
        "colab_type": "text"
      },
      "source": [
        "#### Model loading\n",
        "\n",
        "If you load new models, please do so in a new cell and comment which models are being loaded"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RaTZ5MWPj3Va",
        "colab_type": "code",
        "outputId": "ea40444a-7c6f-426b-a7c7-4e39a7f86d5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318,
          "referenced_widgets": [
            "7bacf37ec1754169ba817424bcbe8afd",
            "57507aaa576748ada0df8483165970ba",
            "d9b2eb920b7d4b3b8ddd266d8afcb638",
            "f3aa5d79d2454952966614ac5f18bcea",
            "e60c40b9f8ba4c1db0c4408dec8ee49a",
            "5ebffae612cf4c35ac5a23d82dbf5498",
            "8805a41eea674c55994978961e40f3f0",
            "61b8e7ab25bc4ff1b05735dfa6f62b64",
            "5ef13396669a4540a044a3deef4be3bd",
            "9d72066db34841b089512a66b14ef620",
            "742b1cd42808450f90b8494c6e6a5fcd",
            "14deb3f99ce84a218ab77902d25cbb7e",
            "7f0f429ff24745f2ba57b7c10a5876ea",
            "eac68c81bd8c451ca2f60b2e199238c9",
            "364600b187bf4404a1c1707aa1471263",
            "7214e5f975864cf6805bbb749f4b8dea",
            "896bf9c054c14fd5bb4417fa10d07b7c",
            "15762e79cd2f40f1a56612e4a4b9d64c",
            "696e6945f71948b2bcad15440a43fb26",
            "0d6293fe3743410ca5d20600265526a2",
            "e2925c97e16143e5991f3d570cae8306",
            "143d521a9337483e9399e45e9e582981",
            "db16a09c645e4d8cbaf8e4b4fb1ef533",
            "1e68ba5d26f9498e9f2383783479ec32",
            "f47b234db09e4d24a215b1d724b8a33d",
            "8456925f82d544ceab63087e3bea237c",
            "9fba6f0fddd245979c0d2944ae562b2c",
            "bf95401a56034146a5df9c33a099ba03",
            "9165c560f9e84030b9e7815e702f0591",
            "493d2fb59a52406a9b2c1b5c070e5d4e",
            "3b07b8e1c87a487c9e3e6ba6bd098124",
            "deb5712b4ede49c499ee0df45d2b0105",
            "2746947dda1c4e36a1430b0088cd56ae",
            "87956815b2884d8297fff8382d2d016a",
            "5924a76b44f2465086c7017de92ae79c",
            "454c7066d854477e9ea2eef6b078a356",
            "7781c3d935e24d5fb950cfd52ac398ef",
            "df59e5787376408285c493e12cdee002",
            "8923c18d1fb44da7a4bd776e1f01d997",
            "d9e1f61447204ad9ba5ce4ac4f8f4c59"
          ]
        }
      },
      "source": [
        "\"\"\"\n",
        "MINI / SMALL / MEDIUM\n",
        "\"\"\"\n",
        "\n",
        "from transformers import BertForQuestionAnswering, BertTokenizer\n",
        "\n",
        "bert_mini_model = BertForQuestionAnswering.from_pretrained('mrm8488/bert-mini-finetuned-squadv2')\n",
        "bert_mini_tokenizer = BertTokenizer.from_pretrained('mrm8488/bert-mini-finetuned-squadv2')\n",
        "print(\"Bert Mini loaded...\")\n",
        "\n",
        "bert_small_model = BertForQuestionAnswering.from_pretrained('mrm8488/bert-small-finetuned-squadv2')\n",
        "bert_small_tokenizer = BertTokenizer.from_pretrained('mrm8488/bert-small-finetuned-squadv2')\n",
        "print(\"Bert Small loaded...\")\n",
        "\n",
        "bert_med_model = BertForQuestionAnswering.from_pretrained('mrm8488/bert-medium-finetuned-squadv2')\n",
        "bert_med_tokenizer = BertTokenizer.from_pretrained('mrm8488/bert-medium-finetuned-squadv2')\n",
        "print(\"Bert Medium loaded...\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7bacf37ec1754169ba817424bcbe8afd",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=463.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5ef13396669a4540a044a3deef4be3bd",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=44701420.0, style=ProgressStyle(descrip…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "896bf9c054c14fd5bb4417fa10d07b7c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f47b234db09e4d24a215b1d724b8a33d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=112.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2746947dda1c4e36a1430b0088cd56ae",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=23.0, style=ProgressStyle(description_w…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Bert Mini loaded...\n",
            "Bert Small loaded...\n",
            "Bert Medium loaded...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0osNIhYXAAc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "SMALL / MEDIUM / XLNET\n",
        "\"\"\"\n",
        "\n",
        "from transformers import BertForQuestionAnswering, BertTokenizer, XLNetForQuestionAnswering, XLNetTokenizer\n",
        "\n",
        "bert_small_model = BertForQuestionAnswering.from_pretrained('mrm8488/bert-small-finetuned-squadv2')\n",
        "bert_small_tokenizer = BertTokenizer.from_pretrained('mrm8488/bert-small-finetuned-squadv2')\n",
        "print(\"Bert Small loaded...\")\n",
        "\n",
        "bert_med_model = BertForQuestionAnswering.from_pretrained('mrm8488/bert-medium-finetuned-squadv2')\n",
        "bert_med_tokenizer = BertTokenizer.from_pretrained('mrm8488/bert-medium-finetuned-squadv2')\n",
        "print(\"Bert Medium loaded...\")\n",
        "\n",
        "XLNET_model = XLNetForQuestionAnswering.from_pretrained('xlnet-base-cased')\n",
        "XLNET_tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
        "print(\"XLNET loaded...\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kDAt5J_xIzb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "SMALL / MEDIUM / DISTIL BASE\n",
        "\"\"\"\n",
        "\n",
        "from transformers import BertForQuestionAnswering, BertTokenizer, DistilBertForQuestionAnswering, DistilBertTokenizer\n",
        "\n",
        "bert_small_model = BertForQuestionAnswering.from_pretrained('mrm8488/bert-small-finetuned-squadv2')\n",
        "bert_small_tokenizer = BertTokenizer.from_pretrained('mrm8488/bert-small-finetuned-squadv2')\n",
        "print(\"Bert Small loaded...\")\n",
        "\n",
        "bert_med_model = BertForQuestionAnswering.from_pretrained('mrm8488/bert-medium-finetuned-squadv2')\n",
        "bert_med_tokenizer = BertTokenizer.from_pretrained('mrm8488/bert-medium-finetuned-squadv2')\n",
        "print(\"Bert Medium loaded...\")\n",
        "\n",
        "distil_bert_model = DistilBertForQuestionAnswering.from_pretrained('distilbert-base-cased')\n",
        "distil_bert_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-cased')\n",
        "print(\"DistilBert loaded...\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQuImBefkret",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "ALBERT\n",
        "\"\"\"\n",
        "\n",
        "from transformers import AlbertTokenizer, AlbertForQuestionAnswering\n",
        "\n",
        "albert_tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')\n",
        "albert_model = AlbertForQuestionAnswering.from_pretrained('albert-base-v2')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEG9PdOPNHEb",
        "colab_type": "text"
      },
      "source": [
        "#### Answer function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxZslsanznku",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def answer_question(question, text, alpha=.5):\n",
        "  \"\"\"\n",
        "  Answering wrapper, change which function you use and which models you use it with\n",
        "  \"\"\"\n",
        "  \n",
        "  return ensemble_triple(bert_mini_model, bert_small_model, bert_med_model, bert_mini_tokenizer, bert_small_tokenizer, bert_med_tokenizer,\n",
        "                        question, text, alpha)\n",
        "  \n",
        "  \n",
        "  \"\"\"\n",
        "  return ensemble_double(bert_mini_model, bert_med_model, bert_mini_tokenizer, bert_med_tokenizer,\n",
        "                        question, text, alpha)\n",
        "  \"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhopLe42NMqX",
        "colab_type": "text"
      },
      "source": [
        "#### Tester code\n",
        "\n",
        "Check if prediction crashes on you."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqic8_YdyIgU",
        "colab_type": "code",
        "outputId": "8278429f-9560-4ea6-d82a-9c028e539e10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "import textwrap\n",
        "\n",
        "# Wrap text to 80 characters.\n",
        "wrapper = textwrap.TextWrapper(width=80) \n",
        "\n",
        "bert_abstract = \"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).\"\n",
        "\n",
        "print(wrapper.fill(bert_abstract))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We introduce a new language representation model called BERT, which stands for\n",
            "Bidirectional Encoder Representations from Transformers. Unlike recent language\n",
            "representation models (Peters et al., 2018a; Radford et al., 2018), BERT is\n",
            "designed to pretrain deep bidirectional representations from unlabeled text by\n",
            "jointly conditioning on both left and right context in all layers. As a result,\n",
            "the pre-trained BERT model can be finetuned with just one additional output\n",
            "layer to create state-of-the-art models for a wide range of tasks, such as\n",
            "question answering and language inference, without substantial taskspecific\n",
            "architecture modifications. BERT is conceptually simple and empirically\n",
            "powerful. It obtains new state-of-the-art results on eleven natural language\n",
            "processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute\n",
            "improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1\n",
            "question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD\n",
            "v2.0 Test F1 to 83.1 (5.1 point absolute improvement).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TuZfMmW3ybKC",
        "colab_type": "code",
        "outputId": "d8b762f6-30cc-4671-d02f-4670e9b46070",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "question = \"What does the 'B' in BERT stand for?\"\n",
        "\n",
        "print(answer_question(question, bert_abstract))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bidirectional encoder representations from transformers\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uubag4QybyP",
        "colab_type": "code",
        "outputId": "75e0e24c-0752-4971-ba65-40954bc1a2bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "question = \"What are some example applications of BERT?\"\n",
        "\n",
        "print(answer_question(question, bert_abstract))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "eleven natural language processing tasks\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTkjiK-v7IS7",
        "colab_type": "text"
      },
      "source": [
        "### Prediction on SQUAD\n",
        "\n",
        "This will tell you how many predictions have failed and will -> \"\" them by default."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtCjoPkt7O2J",
        "colab_type": "code",
        "outputId": "70ec7895-cac0-43a0-d060-53e7e1eef21c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "#COLAB DRIVE MOUNT\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjUjGR1JMr7i",
        "colab_type": "code",
        "outputId": "3ef11bb3-c6b7-43aa-e129-a8adc3a677c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Builtin preprocessor\n",
        "from transformers.data.processors.squad import SquadV2Processor\n",
        "\n",
        "processor = SquadV2Processor()\n",
        "eval_examples = processor.get_dev_examples(\"/content/gdrive/My Drive/squad\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 35/35 [00:03<00:00,  8.83it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgAAQcbMlIIr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The SquadExample class fields\n",
        "\"\"\"\n",
        "self.qas_id = qas_id\n",
        "self.question_text = question_text\n",
        "self.context_text = context_text\n",
        "self.answer_text = answer_text\n",
        "self.title = title\n",
        "self.is_impossible = is_impossible\n",
        "self.answers = answers\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-8iO2bAoj3p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_predictions(eval_set, out_filename, n_examples=None, padding=False):\n",
        "  \"\"\"\n",
        "  Automatic prediction script with options for checkpointing, error coverage,\n",
        "  padding of incomplete experiments and filedumping.\n",
        "  \"\"\"\n",
        "  predictions = dict()\n",
        "  checkpoints = []\n",
        "  error_count = 0\n",
        "  for k in range(1000, 11000, 1000):\n",
        "    # checkpoints.append(k)\n",
        "    pass\n",
        "\n",
        "  if not n_examples:\n",
        "    for i, example in enumerate(eval_set):\n",
        "      try:\n",
        "        predictions[example.qas_id] = answer_question(example.question_text, example.context_text)\n",
        "      except:\n",
        "        predictions[example.qas_id] = \"\"\n",
        "        error_count += 1\n",
        "      output.clear('status_text')\n",
        "      with output.use_tags('status_text'):\n",
        "        print(f\"Loaded {i+1}/{len(eval_set)} Failed predictions: {error_count}\")\n",
        "      if i in checkpoints:\n",
        "        with open('/content/gdrive/My Drive/squad/'+str(int(i/1000))+'k_'+out_filename, 'w') as fp:\n",
        "          json.dump(predictions, fp)\n",
        "        print(f\"Saved checkpoint {i/1000}k\")\n",
        "  else:\n",
        "    for i in range(n_examples):\n",
        "      example = eval_set[i]\n",
        "      predictions[example.qas_id] = answer_question(example.question_text, example.context_text)\n",
        "      output.clear('status_text')\n",
        "      with output.use_tags('status_text'):\n",
        "        print(f\"Loaded {i+1}/{n_examples}\")\n",
        "        \n",
        "    if padding:\n",
        "      for i in range(n_examples, len(eval_set), 1):\n",
        "        example = eval_set[i]\n",
        "        predictions[example.qas_id] = \"\"\n",
        "        output.clear('status_text')\n",
        "        with output.use_tags('status_text'):\n",
        "          print(f\"Loaded {i+1}/{len(eval_set)}\")\n",
        "\n",
        "  output.clear('status_text')\n",
        "\n",
        "  with open('/content/gdrive/My Drive/squad/'+out_filename, 'w') as fp:\n",
        "      json.dump(predictions, fp)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xkjh1QE8pQBd",
        "colab_type": "code",
        "outputId": "6ceee94e-d979-4a81-b6e9-7a76568b4bca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# run_predictions(eval_examples, \"mini_small_med_w2_a5_t33.json\")\n",
        "run_predictions(eval_examples, \"mini_small_med_eq.json\")\n",
        "print(\"Done\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIO_ieDmyJ8j",
        "colab_type": "text"
      },
      "source": [
        "### The eval script for reference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FpTXiGGsrVo1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Official evaluation script for SQuAD version 2.0.\n",
        "\n",
        "In addition to basic functionality, we also compute additional statistics and\n",
        "plot precision-recall curves if an additional na_prob.json file is provided.\n",
        "This file is expected to map question ID's to the model's predicted probability\n",
        "that a question is unanswerable.\n",
        "\"\"\"\n",
        "import argparse\n",
        "import collections\n",
        "import json\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "import sys\n",
        "\n",
        "OPTS = None\n",
        "\n",
        "def parse_args():\n",
        "  parser = argparse.ArgumentParser('Official evaluation script for SQuAD version 2.0.')\n",
        "  parser.add_argument('data_file', metavar='data.json', help='Input data JSON file.')\n",
        "  parser.add_argument('pred_file', metavar='pred.json', help='Model predictions.')\n",
        "  parser.add_argument('--out-file', '-o', metavar='eval.json',\n",
        "                      help='Write accuracy metrics to file (default is stdout).')\n",
        "  parser.add_argument('--na-prob-file', '-n', metavar='na_prob.json',\n",
        "                      help='Model estimates of probability of no answer.')\n",
        "  parser.add_argument('--na-prob-thresh', '-t', type=float, default=1.0,\n",
        "                      help='Predict \"\" if no-answer probability exceeds this (default = 1.0).')\n",
        "  parser.add_argument('--out-image-dir', '-p', metavar='out_images', default=None,\n",
        "                      help='Save precision-recall curves to directory.')\n",
        "  parser.add_argument('--verbose', '-v', action='store_true')\n",
        "  if len(sys.argv) == 1:\n",
        "    parser.print_help()\n",
        "    sys.exit(1)\n",
        "  return parser.parse_args()\n",
        "\n",
        "def make_qid_to_has_ans(dataset):\n",
        "  qid_to_has_ans = {}\n",
        "  for article in dataset:\n",
        "    for p in article['paragraphs']:\n",
        "      for qa in p['qas']:\n",
        "        qid_to_has_ans[qa['id']] = bool(qa['answers'])\n",
        "  return qid_to_has_ans\n",
        "\n",
        "def normalize_answer(s):\n",
        "  \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
        "  def remove_articles(text):\n",
        "    regex = re.compile(r'\\b(a|an|the)\\b', re.UNICODE)\n",
        "    return re.sub(regex, ' ', text)\n",
        "  def white_space_fix(text):\n",
        "    return ' '.join(text.split())\n",
        "  def remove_punc(text):\n",
        "    exclude = set(string.punctuation)\n",
        "    return ''.join(ch for ch in text if ch not in exclude)\n",
        "  def lower(text):\n",
        "    return text.lower()\n",
        "  return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "def get_tokens(s):\n",
        "  if not s: return []\n",
        "  return normalize_answer(s).split()\n",
        "\n",
        "def compute_exact(a_gold, a_pred):\n",
        "  return int(normalize_answer(a_gold) == normalize_answer(a_pred))\n",
        "\n",
        "def compute_f1(a_gold, a_pred):\n",
        "  gold_toks = get_tokens(a_gold)\n",
        "  pred_toks = get_tokens(a_pred)\n",
        "  common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n",
        "  num_same = sum(common.values())\n",
        "  if len(gold_toks) == 0 or len(pred_toks) == 0:\n",
        "    # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n",
        "    return int(gold_toks == pred_toks)\n",
        "  if num_same == 0:\n",
        "    return 0\n",
        "  precision = 1.0 * num_same / len(pred_toks)\n",
        "  recall = 1.0 * num_same / len(gold_toks)\n",
        "  f1 = (2 * precision * recall) / (precision + recall)\n",
        "  return f1\n",
        "\n",
        "def get_raw_scores(dataset, preds):\n",
        "  exact_scores = {}\n",
        "  f1_scores = {}\n",
        "  for article in dataset:\n",
        "    for p in article['paragraphs']:\n",
        "      for qa in p['qas']:\n",
        "        qid = qa['id']\n",
        "        gold_answers = [a['text'] for a in qa['answers']\n",
        "                        if normalize_answer(a['text'])]\n",
        "        if not gold_answers:\n",
        "          # For unanswerable questions, only correct answer is empty string\n",
        "          gold_answers = ['']\n",
        "        if qid not in preds:\n",
        "          print('Missing prediction for %s' % qid)\n",
        "          continue\n",
        "        a_pred = preds[qid]\n",
        "        # Take max over all gold answers\n",
        "        exact_scores[qid] = max(compute_exact(a, a_pred) for a in gold_answers)\n",
        "        f1_scores[qid] = max(compute_f1(a, a_pred) for a in gold_answers)\n",
        "  return exact_scores, f1_scores\n",
        "\n",
        "def apply_no_ans_threshold(scores, na_probs, qid_to_has_ans, na_prob_thresh):\n",
        "  new_scores = {}\n",
        "  for qid, s in scores.items():\n",
        "    pred_na = na_probs[qid] > na_prob_thresh\n",
        "    if pred_na:\n",
        "      new_scores[qid] = float(not qid_to_has_ans[qid])\n",
        "    else:\n",
        "      new_scores[qid] = s\n",
        "  return new_scores\n",
        "\n",
        "def make_eval_dict(exact_scores, f1_scores, qid_list=None):\n",
        "  if not qid_list:\n",
        "    total = len(exact_scores)\n",
        "    return collections.OrderedDict([\n",
        "        ('exact', 100.0 * sum(exact_scores.values()) / total),\n",
        "        ('f1', 100.0 * sum(f1_scores.values()) / total),\n",
        "        ('total', total),\n",
        "    ])\n",
        "  else:\n",
        "    total = len(qid_list)\n",
        "    return collections.OrderedDict([\n",
        "        ('exact', 100.0 * sum(exact_scores[k] for k in qid_list) / total),\n",
        "        ('f1', 100.0 * sum(f1_scores[k] for k in qid_list) / total),\n",
        "        ('total', total),\n",
        "    ])\n",
        "\n",
        "def merge_eval(main_eval, new_eval, prefix):\n",
        "  for k in new_eval:\n",
        "    main_eval['%s_%s' % (prefix, k)] = new_eval[k]\n",
        "\n",
        "def plot_pr_curve(precisions, recalls, out_image, title):\n",
        "  plt.step(recalls, precisions, color='b', alpha=0.2, where='post')\n",
        "  plt.fill_between(recalls, precisions, step='post', alpha=0.2, color='b')\n",
        "  plt.xlabel('Recall')\n",
        "  plt.ylabel('Precision')\n",
        "  plt.xlim([0.0, 1.05])\n",
        "  plt.ylim([0.0, 1.05])\n",
        "  plt.title(title)\n",
        "  plt.savefig(out_image)\n",
        "  plt.clf()\n",
        "\n",
        "def make_precision_recall_eval(scores, na_probs, num_true_pos, qid_to_has_ans,\n",
        "                               out_image=None, title=None):\n",
        "  qid_list = sorted(na_probs, key=lambda k: na_probs[k])\n",
        "  true_pos = 0.0\n",
        "  cur_p = 1.0\n",
        "  cur_r = 0.0\n",
        "  precisions = [1.0]\n",
        "  recalls = [0.0]\n",
        "  avg_prec = 0.0\n",
        "  for i, qid in enumerate(qid_list):\n",
        "    if qid_to_has_ans[qid]:\n",
        "      true_pos += scores[qid]\n",
        "    cur_p = true_pos / float(i+1)\n",
        "    cur_r = true_pos / float(num_true_pos)\n",
        "    if i == len(qid_list) - 1 or na_probs[qid] != na_probs[qid_list[i+1]]:\n",
        "      # i.e., if we can put a threshold after this point\n",
        "      avg_prec += cur_p * (cur_r - recalls[-1])\n",
        "      precisions.append(cur_p)\n",
        "      recalls.append(cur_r)\n",
        "  if out_image:\n",
        "    plot_pr_curve(precisions, recalls, out_image, title)\n",
        "  return {'ap': 100.0 * avg_prec}\n",
        "\n",
        "def run_precision_recall_analysis(main_eval, exact_raw, f1_raw, na_probs, \n",
        "                                  qid_to_has_ans, out_image_dir):\n",
        "  if out_image_dir and not os.path.exists(out_image_dir):\n",
        "    os.makedirs(out_image_dir)\n",
        "  num_true_pos = sum(1 for v in qid_to_has_ans.values() if v)\n",
        "  if num_true_pos == 0:\n",
        "    return\n",
        "  pr_exact = make_precision_recall_eval(\n",
        "      exact_raw, na_probs, num_true_pos, qid_to_has_ans,\n",
        "      out_image=os.path.join(out_image_dir, 'pr_exact.png'),\n",
        "      title='Precision-Recall curve for Exact Match score')\n",
        "  pr_f1 = make_precision_recall_eval(\n",
        "      f1_raw, na_probs, num_true_pos, qid_to_has_ans,\n",
        "      out_image=os.path.join(out_image_dir, 'pr_f1.png'),\n",
        "      title='Precision-Recall curve for F1 score')\n",
        "  oracle_scores = {k: float(v) for k, v in qid_to_has_ans.items()}\n",
        "  pr_oracle = make_precision_recall_eval(\n",
        "      oracle_scores, na_probs, num_true_pos, qid_to_has_ans,\n",
        "      out_image=os.path.join(out_image_dir, 'pr_oracle.png'),\n",
        "      title='Oracle Precision-Recall curve (binary task of HasAns vs. NoAns)')\n",
        "  merge_eval(main_eval, pr_exact, 'pr_exact')\n",
        "  merge_eval(main_eval, pr_f1, 'pr_f1')\n",
        "  merge_eval(main_eval, pr_oracle, 'pr_oracle')\n",
        "\n",
        "def histogram_na_prob(na_probs, qid_list, image_dir, name):\n",
        "  if not qid_list:\n",
        "    return\n",
        "  x = [na_probs[k] for k in qid_list]\n",
        "  weights = np.ones_like(x) / float(len(x))\n",
        "  plt.hist(x, weights=weights, bins=20, range=(0.0, 1.0))\n",
        "  plt.xlabel('Model probability of no-answer')\n",
        "  plt.ylabel('Proportion of dataset')\n",
        "  plt.title('Histogram of no-answer probability: %s' % name)\n",
        "  plt.savefig(os.path.join(image_dir, 'na_prob_hist_%s.png' % name))\n",
        "  plt.clf()\n",
        "\n",
        "def find_best_thresh(preds, scores, na_probs, qid_to_has_ans):\n",
        "  num_no_ans = sum(1 for k in qid_to_has_ans if not qid_to_has_ans[k])\n",
        "  cur_score = num_no_ans\n",
        "  best_score = cur_score\n",
        "  best_thresh = 0.0\n",
        "  qid_list = sorted(na_probs, key=lambda k: na_probs[k])\n",
        "  for i, qid in enumerate(qid_list):\n",
        "    if qid not in scores: continue\n",
        "    if qid_to_has_ans[qid]:\n",
        "      diff = scores[qid]\n",
        "    else:\n",
        "      if preds[qid]:\n",
        "        diff = -1\n",
        "      else:\n",
        "        diff = 0\n",
        "    cur_score += diff\n",
        "    if cur_score > best_score:\n",
        "      best_score = cur_score\n",
        "      best_thresh = na_probs[qid]\n",
        "  return 100.0 * best_score / len(scores), best_thresh\n",
        "\n",
        "def find_all_best_thresh(main_eval, preds, exact_raw, f1_raw, na_probs, qid_to_has_ans):\n",
        "  best_exact, exact_thresh = find_best_thresh(preds, exact_raw, na_probs, qid_to_has_ans)\n",
        "  best_f1, f1_thresh = find_best_thresh(preds, f1_raw, na_probs, qid_to_has_ans)\n",
        "  main_eval['best_exact'] = best_exact\n",
        "  main_eval['best_exact_thresh'] = exact_thresh\n",
        "  main_eval['best_f1'] = best_f1\n",
        "  main_eval['best_f1_thresh'] = f1_thresh\n",
        "\n",
        "def main():\n",
        "  with open(OPTS.data_file) as f:\n",
        "    dataset_json = json.load(f)\n",
        "    dataset = dataset_json['data']\n",
        "  with open(OPTS.pred_file) as f:\n",
        "    preds = json.load(f)\n",
        "  if OPTS.na_prob_file:\n",
        "    with open(OPTS.na_prob_file) as f:\n",
        "      na_probs = json.load(f)\n",
        "  else:\n",
        "    na_probs = {k: 0.0 for k in preds}\n",
        "  qid_to_has_ans = make_qid_to_has_ans(dataset)  # maps qid to True/False\n",
        "  has_ans_qids = [k for k, v in qid_to_has_ans.items() if v]\n",
        "  no_ans_qids = [k for k, v in qid_to_has_ans.items() if not v]\n",
        "  exact_raw, f1_raw = get_raw_scores(dataset, preds)\n",
        "  exact_thresh = apply_no_ans_threshold(exact_raw, na_probs, qid_to_has_ans,\n",
        "                                        OPTS.na_prob_thresh)\n",
        "  f1_thresh = apply_no_ans_threshold(f1_raw, na_probs, qid_to_has_ans,\n",
        "                                     OPTS.na_prob_thresh)\n",
        "  out_eval = make_eval_dict(exact_thresh, f1_thresh)\n",
        "  if has_ans_qids:\n",
        "    has_ans_eval = make_eval_dict(exact_thresh, f1_thresh, qid_list=has_ans_qids)\n",
        "    merge_eval(out_eval, has_ans_eval, 'HasAns')\n",
        "  if no_ans_qids:\n",
        "    no_ans_eval = make_eval_dict(exact_thresh, f1_thresh, qid_list=no_ans_qids)\n",
        "    merge_eval(out_eval, no_ans_eval, 'NoAns')\n",
        "  if OPTS.na_prob_file:\n",
        "    find_all_best_thresh(out_eval, preds, exact_raw, f1_raw, na_probs, qid_to_has_ans)\n",
        "  if OPTS.na_prob_file and OPTS.out_image_dir:\n",
        "    run_precision_recall_analysis(out_eval, exact_raw, f1_raw, na_probs, \n",
        "                                  qid_to_has_ans, OPTS.out_image_dir)\n",
        "    histogram_na_prob(na_probs, has_ans_qids, OPTS.out_image_dir, 'hasAns')\n",
        "    histogram_na_prob(na_probs, no_ans_qids, OPTS.out_image_dir, 'noAns')\n",
        "  if OPTS.out_file:\n",
        "    with open(OPTS.out_file, 'w') as f:\n",
        "      json.dump(out_eval, f)\n",
        "  else:\n",
        "    print(json.dumps(out_eval, indent=2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPeTWwSursk2",
        "colab_type": "text"
      },
      "source": [
        "### Code dump\n",
        "\n",
        "Contains unused or removed code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nTeRUgsrxiY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    \"\"\"\n",
        "    # ======== Tokenize ========\n",
        "    # Apply the tokenizer to the input text, treating them as a text-pair.\n",
        "    input_ids_x = tokenizer_x.encode(question, answer_text)\n",
        "    input_ids_y = tokenizer_y.encode(question, answer_text)\n",
        "    \n",
        "    encoded_dict_x = tokenizer_x.encode_plus(text=question, text_pair=answer_text, add_special=True)\n",
        "    input_ids_x = encoded_dict_x['token_type_ids']\n",
        "\n",
        "    encoded_dict_y = tokenizer_y.encode_plus(text=question, text_pair=answer_text, add_special=True)\n",
        "    input_ids_y = encoded_dict_y['token_type_ids']\n",
        "    \n",
        "    print(f\"X: {len(input_ids_x)}, Y: {len(input_ids_y)}\")\n",
        "\n",
        "    # ======== Set Segment IDs ========\n",
        "    # Search the input_ids for the first instance of the `[SEP]` token.\n",
        "    sep_index_x = input_ids_x.index(tokenizer_x.sep_token_id)\n",
        "    sep_index_y = input_ids_y.index(tokenizer_y.sep_token_id)\n",
        "\n",
        "    # The number of segment A tokens includes the [SEP] token istelf.\n",
        "    num_seg_a_x = sep_index_x + 1\n",
        "    num_seg_a_y = sep_index_y + 1\n",
        "\n",
        "    # The remainder are segment B.\n",
        "    num_seg_b_x = len(input_ids_x) - num_seg_a_x\n",
        "    num_seg_b_y = len(input_ids_y) - num_seg_a_y\n",
        "\n",
        "    # Construct the list of 0s and 1s.\n",
        "    segment_ids_x = [0]*num_seg_a_x + [1]*num_seg_b_x\n",
        "    segment_ids_y = [0]*num_seg_a_y + [1]*num_seg_b_y\n",
        "\n",
        "    # There should be a segment_id for every input token.\n",
        "    assert len(segment_ids_x) == len(input_ids_x)\n",
        "    assert len(segment_ids_y) == len(input_ids_y)\n",
        "\n",
        "    # ======== Evaluate ========\n",
        "    # Run our example question through the model.\n",
        "    start_scores_x, end_scores_x = model_x(torch.tensor([input_ids_x]), # The tokens representing our input text.\n",
        "                                    token_type_ids=torch.tensor([segment_ids_x])) # The segment IDs to differentiate question from answer_text\n",
        "    \n",
        "    start_scores_y, end_scores_y = model_y(torch.tensor([input_ids_y]), \n",
        "                                    token_type_ids=torch.tensor([segment_ids_y]))\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    \n",
        "    \"\"\"\n",
        "    # Get the string versions of the input tokens.\n",
        "    tokens = tokenizer_x.convert_ids_to_tokens(input_ids_x)\n",
        "\n",
        "    # Start with the first token.\n",
        "    answer = tokens[answer_start]\n",
        "\n",
        "    # Select the remaining answer tokens and join them with whitespace.\n",
        "    for i in range(answer_start + 1, answer_end + 1):\n",
        "        \n",
        "        # If it's a subword token, then recombine it with the previous token.\n",
        "        if tokens[i][0:2] == '##':\n",
        "            answer += tokens[i][2:]\n",
        "        \n",
        "        # Otherwise, add a space then the token.\n",
        "        else:\n",
        "            answer += ' ' + tokens[i]\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    from transformers import squad_convert_examples_to_features\n",
        "    # Convert to dataset\n",
        "    eval_dataset_bert = squad_convert_examples_to_features(\n",
        "        examples=eval_examples,\n",
        "        tokenizer=bert_tokenizer,\n",
        "        max_seq_length=384,\n",
        "        doc_stride=128,\n",
        "        max_query_length=64,\n",
        "        is_training=False,\n",
        "        return_dataset=\"pt\",\n",
        "        )\n",
        "\n",
        "    eval_dataset_albert = squad_convert_examples_to_features(\n",
        "        examples=eval_examples,\n",
        "        tokenizer=albert_tokenizer,\n",
        "        max_seq_length=384,\n",
        "        doc_stride=128,\n",
        "        max_query_length=64,\n",
        "        is_training=False,\n",
        "        return_dataset=\"pt\",\n",
        "        )     \n",
        "    \"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}