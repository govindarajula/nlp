{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"step_2.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"aUxwG-8FUS-0","colab_type":"code","colab":{}},"source":[" import sys\n","sys.path.append('drive/My Drive/NLP_project/Project_2/')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lk-EYCqnW3dU","colab_type":"code","outputId":"d9f36c16-2fb5-430e-ce85-cb9fb705786e","executionInfo":{"status":"ok","timestamp":1582349064594,"user_tz":360,"elapsed":4806,"user":{"displayName":"Nikita Raut","photoUrl":"","userId":"01894045538115627606"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["!pip install import-ipynb\n","import import_ipynb"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: import-ipynb in /usr/local/lib/python3.6/dist-packages (0.1.3)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"EU2KQE1BXGZJ","colab_type":"code","colab":{}},"source":["!pip install -U -q PyDrive\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1EVxgGDgXNfH","colab_type":"code","colab":{}},"source":["auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vj_DV5nJXdmp","colab_type":"code","colab":{}},"source":["your_module = drive.CreateFile({'id':'1ZIKSz87M3yaMvwhunh4Q_cDcW49Y8Mbu'})\n","your_module.GetContentFile('step_1.ipynb')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mWaHa7_Rl74t","colab_type":"code","colab":{}},"source":["your_module = drive.CreateFile({'id':'1ZIKSz87M3yaMvwhunh4Q_cDcW49Y8Mbu'})\n","your_module.GetContentFile('step_1.ipynb')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"00e6ba86-7a46-4177-9ca2-bcc931220029","executionInfo":{"status":"error","timestamp":1582349075889,"user_tz":360,"elapsed":1149,"user":{"displayName":"Nikita Raut","photoUrl":"","userId":"01894045538115627606"}},"id":"Blos14oSisem","colab":{"base_uri":"https://localhost:8080/","height":283}},"source":["import numpy as np\n","from step_1 import load_pickle\n","# from step_1 import save_pickle\n","import warnings\n","from step_1 import create_batches\n","import math\n","from tqdm import tqdm\n","import time\n","import matplotlib.pyplot as plt\n","\n","\n","with warnings.catch_warnings():\n","    warnings.filterwarnings(\"ignore\")\n","    import tensorflow as tf\n","    print(tf.__version__)\n","\n","\n","class Parameters():\n","    def __init__(self, V, n_train, n_valid, n_test):\n","        self.window_size = 2\n","        self.n_nodes_hl = 100\n","        self.n_dimensions = 100\n","        self.alpha = 0.003\n","        self.n_batches = 30\n","        self.batch_size = math.ceil(n_train / self.n_batches)\n","        self.batch_size_valid = math.ceil(n_valid / self.n_batches)\n","        self.batch_size_test = math.ceil(n_test / self.n_batches)\n","        self.in_size = [self.n_batches, self.window_size * self.n_dimensions]  #no of batches = 668921\n","        self.V = V\n","        self.out_size = [self.n_batches, V]\n","        self.n_epochs = 20\n","        self.r = 0.1\n","\n","\n","class BengioModel:\n","    def __init__(self, params):\n","        self.x = tf.compat.v1.placeholder(tf.int32, shape=[params.n_batches, params.window_size], name=\"x\")\n","        self.y = tf.compat.v1.placeholder(tf.int32, shape=[params.n_batches], name=\"y\")\n","        self.alpha = tf.compat.v1.placeholder(tf.float32, shape=[1], name=\"alpha\")\n","        with tf.compat.v1.variable_scope(\"foo\", reuse = tf.AUTO_REUSE) as scope:\n","            self.C = tf.compat.v1.get_variable(\"C\", [params.V, params.n_dimensions], tf.float32)\n","            # self.W = tf.compat.v1.get_variable(\"W\", [params.window_size * params.n_dimensions, params.V], tf.float32)\n","            self.H = tf.compat.v1.get_variable(\"H\", [params.window_size * params.n_dimensions, params.n_nodes_hl], tf.float32)\n","            self.d = tf.compat.v1.get_variable(\"d\", [1, params.n_nodes_hl], tf.float32)\n","            self.U = tf.compat.v1.get_variable(\"U\", [params.n_nodes_hl, params.V], tf.float32)\n","            # self.b = tf.compat.v1.get_variable(\"b\", [1, params.V], tf.float32)\n","\n","    def forward(self, x):\n","        x_embedded = tf.reshape(tf.nn.embedding_lookup(self.C, x), params.in_size)\n","        # x_embedded = tf.reshape(tf.nn.embedding_lookup(self.C, x), params.in_size)\n","        a1 = tf.matmul(tf.tanh(tf.add(self.d, (tf.matmul(x_embedded, self.H)))), self.U)\n","        direct = tf.add(tf.add(tf.matmul(x_embedded, self.W), self.b), a1)\n","        # yhat = tf.nn.softmax(a1)\n","        return direct\n","        # return tf.math.reduce_max(yhat, axis = 1)\n","        # for i in y:\n","        #   vocalb_dict[y]\n","    \n","    def compute_loss(self, logits, y,vocab_size):\n","      y_labels = tf.one_hot(y, vocab_size, name='y_label')\n","      self.loss = tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_labels, logits=logits)\n","\n","    def perform_optimization(self, alpha, loss):\n","      self.optimizer = tf.train.GradientDescentOptimizer(alpha).minimize(loss)\n","\n","\n","def run_model(data, valid_data, test_data, model, running_mode):\n","\n","\n","    if running_mode == 'train':\n","        train_data = create_batches(data, params.n_batches)\n","        valid_data = create_batches(valid_data, params.n_batches)\n","        test_data = create_batches(test_data, params.n_batches)\n","\n","\n","        logits = model.forward(model.x)\n","        model.compute_loss(logits, model.y, params.V)\n","        model.perform_optimization(params.alpha, model.loss)\n","        \n","        # yhat = model.forward(model.x)\n","        # y_hat = tf.convert_to_tensor([yhat[_, tf.cast(model.y[_], tf.int32)] for _ in range(params.n_batches)])\n","        # cost = -tf.math.divide(tf.math.reduce_sum(tf.math.log(y_hat)), params.n_batches)\n","        # optimizer = tf.train.GradientDescentOptimizer(params.alpha).minimize(cost)\n","        \n","        # y_np = np.empty((params.n_batches, 0))\n","        # y_np_valid = np.empty((params.n_batches, 0))\n","        perplexity = []\n","        perplexity_valid = []\n","        perplexity_test = []\n","        saver = tf.train.Saver()\n","\n","        with tf.Session() as sess:\n","            sess.run(tf.initialize_all_variables())\n","            for epoch in range(params.n_epochs):\n","                start = time.time()\n","\n","                y_np, y_np_valid, y_np_test = [], [], []\n","                epoch_loss, epoch_loss_valid, epoch_loss_test = 0, 0, 0\n","\n","                # Train\n","                for i in range(params.batch_size - params.window_size):\n","                    x = train_data[:, i: i+params.window_size]\n","                    y = train_data[:, i+params.window_size]\n","                    _, c = sess.run([model.optimizer, model.loss], feed_dict={model.x: x, model.y: y})\n","                    epoch_loss += np.mean(c)\n","                perplexity.append(math.exp(epoch_loss/(params.batch_size - params.window_size)))\n","\n","                # Validate\n","                for i in range(params.batch_size_valid - params.window_size):\n","                    x_valid = valid_data[:, i: i+params.window_size]\n","                    y_valid = valid_data[:, i+params.window_size]\n","                    c_valid = sess.run([model.loss], feed_dict={model.x: x_valid, model.y: y_valid})\n","                    epoch_loss_valid += np.mean(c_valid)\n","                perplexity_valid.append(math.exp(epoch_loss_valid/(params.batch_size_valid - params.window_size)))\n","                \n","\n","                # Test\n","                for i in range(params.batch_size_test - params.window_size):\n","                    x_test = test_data[:, i: i+params.window_size]\n","                    y_test = test_data[:, i+params.window_size]\n","                    c_test = sess.run([model.loss], feed_dict={model.x: x_test, model.y: y_test})\n","                    epoch_loss_test += np.mean(c_test)\n","                perplexity_test.append(math.exp(epoch_loss_test/(params.batch_size_test - params.window_size)))\n","\n","                \n","                print(\"Epoch: {}  Training loss: {}   Validation loss: {}   Testing loss: {}   Training perplexity: {}   Validation perplexity: {}   Test perplexity: {}  Time: {}\".format(\n","                    epoch, epoch_loss, epoch_loss_valid, epoch_loss_test, \n","                    perplexity[-1], perplexity_valid[-1], perplexity_test[-1], \n","                    time.time()-start))\n","            \n","            saver.save(sess, 'drive/My Drive/Winter 2020/NLP/NLP_project/Project_2/data/wiki_win-5')\n","        plt.plot(perplexity)\n","        plt.plot(perplexity_valid)\n","        plt.plot(perplexity_test)\n","        plt.legend(['Train', 'Validation', 'Test'])\n","        plt.ylabel('perplexity')\n","        plt.xlabel('epochs')\n","        plt.show()\n","\n","if __name__ == \"__main__\":\n","    # vocab_dict = load_pickle(\"vocab_dict_brown.pkl\")\n","    # train_data = load_pickle(\"brown/train_step3.pkl\")\n","    # #train_data = train_data[:200]\n","    # # print(len(train_data))\n","    # # train_data = train_data[:20000]\n","    # params = Parameters(len(vocab_dict), len(train_data))\n","    # bengio_model = BengioModel(params)\n","    # run_model(train_data, bengio_model, 'train')\n","\n","    vocab_dict = load_pickle(\"vocab_dict_brown.pkl\")\n","    train_data = load_pickle(\"brown/train_step3.pkl\")\n","    valid_data = load_pickle(\"brown/valid_step3.pkl\")\n","    test_data = load_pickle(\"brown/test_step3.pkl\")\n","    print('Train data lenght: {}'.format(len(train_data)))\n","    # train_data = train_data[:5_000_000]\n","    # valid_data = valid_data[:500]\n","    # test_data = test_data[:500]\n","    params = Parameters(len(vocab_dict), len(train_data), len(valid_data), len(test_data))\n","    bengio_model = BengioModel(params)\n","    run_model(train_data, valid_data, test_data, bengio_model, 'train')\n","    # run_model(test_data, None, bengio_model, 'test')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["1.15.0\n"],"name":"stdout"},{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-1ef1c1414b34>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;31m# run_model(train_data, bengio_model, 'train')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m     \u001b[0mvocab_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"vocab_dict_brown.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"brown/train_step3.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0mvalid_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"brown/valid_step3.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/step_1.ipynb\u001b[0m in \u001b[0;36mload_pickle\u001b[0;34m(filename)\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'drive/My Drive/NLP_project/Project_2/data/vocab_dict_brown.pkl'"]}]},{"cell_type":"code","metadata":{"id":"7sjXU9CJq8A2","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}